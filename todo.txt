
Only do reduction of best results across agents groups, not across the trials.  After getting the
best result for each agent group, take the average over all the trials.

improve number of threads per block for calc_quality kernel

Setup competition to only compete against a certain number of others, such as 8, 16, or 32.  Then the winner of each group will have their fitness calc'd to see if they are the new best.  Sharing of the new best will be the same as current.


Create visual learning tool to see the learning level of all the agents as they progress.
Show agents dieing, being cloned, etc.  Possibly add sliders for changing the learning parameters
for alpha, test-interval, add more agents

Investigate why fitness values change slightly.  Is Rand... function being called from some testing routing?

Reduce register usage



Have 2 different share_best_pct, one for when a new agent becomes the best and one for every other time.  Could then do away with the --SHARE_ALWAYS flag.



increse BLOCK_SIZE for kernels that don't use as much shared memory


log time of tests to test data

save best agent (or selected agent) weights after each test


When calc'ing all agent's quality, instead of adding 0.5f to the division number, add a random value between 0.0 and 1.0

In CALC_QUALITY kernels, is it better to have large block size and then calculate the x_div_num and _vel_div_num
values, or is it better to have threadIdx.x = the x_div_num and blockIdx.x = vel_div_num

Set alpha with a random distribution for copies of the best agent.  Some alphas can be larger
than normal and some smaller. (Or even change lambda, wgts, or something else)


Add a minimum fitness delta value so the best agent does not change unless the new agent's fitness exceeds
current best by a certain threshold.

Store the best agents weights in the GPU result data, and then let the agent continue to evolve.

Add a separate share best % to be used when there is not a new best agent.
